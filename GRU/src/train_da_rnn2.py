import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import torch.nn.functional as F
import os
import sys

# ---------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------
# Mac(M1/M2) ì‚¬ìš©ì‹œ 'mps' ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ 'cpu'
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜(Device): {device}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
HIDDEN_SIZE = 64
LEARNING_RATE = 0.001 # í•™ìŠµì´ ë„ˆë¬´ ë¹ ë¥´ë©´ 0.0005ë¡œ ì¤„ì´ì„¸ìš”
BATCH_SIZE = 64
EPOCHS = 50           # ë°ì´í„°ê°€ ì¤„ì–´ì„œ Epochë¥¼ ë„‰ë„‰íˆ ì£¼ì…”ë„ ë¹ ë¦…ë‹ˆë‹¤

def load_data():
    print("\nğŸ“‚ Max Pooling ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        # 1. Max Poolingëœ X ë°ì´í„° ë¡œë“œ
        X_train = np.load('X_train_maxpool.npy')
        X_test = np.load('X_test_maxpool.npy')
        
        # 2. y ë°ì´í„° ë¡œë“œ (yëŠ” pooling ì˜í–¥ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ í˜¹ì€ ë³µì‚¬ë³¸ ì‚¬ìš©)
        # ë§Œì•½ y_train.npyê°€ ì—†ë‹¤ë©´ ì›ë³¸ ìƒì„± ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.
        if os.path.exists('y_train.npy'):
            y_train = np.load('y_train.npy')
        else:
            # y_trainì´ ì—†ìœ¼ë©´ ì„ì‹œë°©í¸ìœ¼ë¡œ y_testì™€ ê°™ì€ ë¡œì§ìœ¼ë¡œ ê°€ì • (ì‹¤ì œë¡œëŠ” y_train í•„ìˆ˜)
            print("âš ï¸ ê²½ê³ : 'y_train.npy'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ í´ë” í™•ì¸ í•„ìš”.")
            sys.exit()

        if os.path.exists('y_test_maxpool.npy'):
            y_test = np.load('y_test_maxpool.npy')
        else:
            y_test = np.load('y_test.npy')

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        print(f"   X_train: {X_train.shape} (Time Stepì´ ì¤„ì–´ë“¤ì—ˆëŠ”ì§€ í™•ì¸!)")
        print(f"   y_train: {y_train.shape}")
        
        return X_train, y_train, X_test, y_test

    except FileNotFoundError as e:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. {e}")
        sys.exit()

X_train_np, y_train_np, X_test_np, y_test_np = load_data()

# Tensor ë³€í™˜
train_data = TensorDataset(torch.FloatTensor(X_train_np), torch.FloatTensor(y_train_np))
test_data = TensorDataset(torch.FloatTensor(X_test_np), torch.FloatTensor(y_test_np))

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

# ì…ë ¥ ì°¨ì› ìë™ ì„¤ì • (Feature ê°œìˆ˜)
INPUT_SIZE = X_train_np.shape[2] 

# ---------------------------------------------------------
# 2. ëª¨ë¸ ì •ì˜ (ë“¤ì—¬ì“°ê¸° ìˆ˜ì • ì™„ë£Œë¨)
# ---------------------------------------------------------
class InputAttention(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(InputAttention, self).__init__()
        self.linear = nn.Linear(hidden_size + input_size, input_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x_t, h_prev):
        # x_t: (Batch, Input), h_prev: (Batch, Hidden)
        combined = torch.cat((x_t, h_prev), dim=1)
        scores = torch.tanh(self.linear(combined))
        alpha = self.softmax(scores) 
        return alpha * x_t, alpha

class TemporalAttention(nn.Module):
    def __init__(self, hidden_size):
        super(TemporalAttention, self).__init__()
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, encoder_outputs):
        # encoder_outputs: (Batch, Seq, Hidden)
        scores = torch.tanh(self.linear(encoder_outputs))
        beta = F.softmax(scores, dim=1) 
        context = torch.sum(beta * encoder_outputs, dim=1)
        return context, beta

class DA_RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DA_RNN, self).__init__()
        self.hidden_size = hidden_size
        
        # 1. Input Attention + Encoder
        self.input_attention = InputAttention(input_size, hidden_size)
        self.gru_cell = nn.GRUCell(input_size, hidden_size)
        
        # 2. Temporal Attention
        self.temporal_attention = TemporalAttention(hidden_size)
        
        # 3. Classifier
        self.fc = nn.Linear(hidden_size, output_size)
    
    # [ìˆ˜ì •ë¨] forwardê°€ __init__ ë°–ìœ¼ë¡œ ë‚˜ì™”ìŠµë‹ˆë‹¤.
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
        encoder_outputs = []
        
        # [Stage 1] Loop over time steps
        for t in range(seq_len):
            x_t = x[:, t, :]
            weighted_x_t, _ = self.input_attention(x_t, h_t)
            h_t = self.gru_cell(weighted_x_t, h_t)
            encoder_outputs.append(h_t.unsqueeze(1))
            
        encoder_outputs = torch.cat(encoder_outputs, dim=1)
        
        # [Stage 2] Temporal Attention
        context_vector, _ = self.temporal_attention(encoder_outputs)
        
        # [Stage 3] Prediction
        logits = self.fc(context_vector)
        return torch.sigmoid(logits)

# ---------------------------------------------------------
# 3. í•™ìŠµ ë£¨í”„ (Training Loop)
# ---------------------------------------------------------
model = DA_RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=1).to(device)
criterion = nn.BCELoss() # ëª¨ë¸ì´ sigmoidë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ BCELoss ì‚¬ìš©
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print(f"\nğŸ”¥ í•™ìŠµ ì‹œì‘! (Epochs: {EPOCHS})")
print("-" * 60)

train_losses = []
best_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        
        # ì°¨ì› ë§ì¶”ê¸° (Batch, 1) -> (Batch,)
        outputs = outputs.squeeze()
        loss = criterion(outputs, targets)
        
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_loss)
    
    # ì§„í–‰ ìƒí™© ì¶œë ¥ (5 epoch ë§ˆë‹¤)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.5f}")

    # Best Model ì €ì¥
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), 'da_rnn_best_maxpool.pth')

print("-" * 60)
print(f"âœ… í•™ìŠµ ì™„ë£Œ! Best Loss: {best_loss:.5f}")
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: da_rnn_best_maxpool.pth")

# ---------------------------------------------------------
# 4. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
# ---------------------------------------------------------
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss', color='blue')
plt.title('DA-RNN Training Loss (with Max Pooling Data)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()