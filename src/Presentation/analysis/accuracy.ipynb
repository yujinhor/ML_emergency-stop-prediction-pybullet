{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwtC+I4SYYSYjxUj4JZxTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yujinhor/ML_emergency-stop-prediction-pybullet/blob/main/accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUvdsgn_ef7A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ“‚ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ì¤‘...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Robot_Analysis'\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "PRED_PATH = os.path.join(BASE_DIR, 'danger_max_duration_summary.csv')  # AI ì˜ˆì¸¡ íŒŒì¼\n",
        "LABEL_PATH = os.path.join(BASE_DIR, 'val_summary.csv')                 # ì‹¤ì œ ì •ë‹µ íŒŒì¼\n",
        "OUTPUT_PATH = os.path.join(BASE_DIR, 'simple_match_result.csv')        # ê²°ê³¼ ì €ì¥\n",
        "\n",
        "def main():\n",
        "    print(\"ğŸ“– ë°ì´í„° ì½ëŠ” ì¤‘...\")\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    df_pred = pd.read_csv(PRED_PATH)\n",
        "    df_label = pd.read_csv(LABEL_PATH)\n",
        "\n",
        "    # 2. ë”± í•„ìš”í•œ ì •ë³´ë§Œ ë½‘ì•„ì„œ ë³‘í•© (Episode_ID ê¸°ì¤€)\n",
        "    # AI íŒŒì¼ì—ëŠ” 'Episode_ID', ì •ë‹µ íŒŒì¼ì—ëŠ” 'episode'ë¡œ ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ\n",
        "\n",
        "    # ì •ë‹µ íŒŒì¼ì—ì„œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ì™€ ê²°ê³¼(result_is_failure)ë§Œ ê°€ì ¸ì˜´\n",
        "    # (result_is_failure: 1=ê³ ì¥, 0=ì„±ê³µ)\n",
        "    label_subset = df_label[['episode', 'result_is_failure']].copy()\n",
        "    label_subset.rename(columns={'episode': 'Episode_ID'}, inplace=True)\n",
        "\n",
        "    # ë³‘í•© (Left Join: AIê°€ ìœ„í—˜í•˜ë‹¤ê³  í•œ ëª©ë¡ ê¸°ì¤€)\n",
        "    merged_df = pd.merge(df_pred[['Episode_ID', 'Avg_Prob']], label_subset, on='Episode_ID', how='left')\n",
        "\n",
        "    # 3. ê²°ê³¼ íŒì • (O/X ì±„ì )\n",
        "    # AIëŠ” ì´ë¯¸ ìœ„í—˜í•˜ë‹¤ê³  ì˜ˆì¸¡í–ˆìœ¼ë¯€ë¡œ, ì‹¤ì œê°€ 1ì´ë©´ ì •ë‹µ, 0ì´ë©´ í‹€ë¦° ê²ƒì„\n",
        "    merged_df['Check'] = merged_df['result_is_failure'].apply(\n",
        "        lambda x: 'O (ì •ë‹µ)' if x == 1 else 'X (ì˜¤ì§„)'\n",
        "    )\n",
        "\n",
        "    # ë³´ê¸° ì¢‹ê²Œ ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
        "    merged_df.columns = ['ì—í”¼ì†Œë“œ_ë²ˆí˜¸', 'AI_ì˜ˆì¸¡í™•ë¥ ', 'ì‹¤ì œ_ê²°ê³¼(1=ê³ ì¥)', 'íŒì •']\n",
        "\n",
        "    # 4. ì €ì¥ ë° ì¶œë ¥\n",
        "    merged_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"\\nâœ… ë§¤ì¹­ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {OUTPUT_PATH}\")\n",
        "    print(\"\\n[ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°]\")\n",
        "    print(merged_df.head(10)) # ìƒìœ„ 10ê°œ ì¶œë ¥\n",
        "\n",
        "    # 5. ê°„ë‹¨ ìš”ì•½\n",
        "    correct = len(merged_df[merged_df['ì‹¤ì œ_ê²°ê³¼(1=ê³ ì¥)'] == 1])\n",
        "    total = len(merged_df)\n",
        "    print(f\"\\nğŸ“Š ìš”ì•½: AIê°€ ìœ„í—˜í•˜ë‹¤ê³  í•œ {total}ê°œ ì¤‘ {correct}ê°œê°€ ì‹¤ì œë¡œ ê³ ì¥ë‚¨.\")\n",
        "    print(f\"   -> ì ì¤‘ë¥ : {(correct/total)*100:.1f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-d0FnlVLuC",
        "outputId": "59e85756-2500-4294-9505-7950872017b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“– ë°ì´í„° ì½ëŠ” ì¤‘...\n",
            "\n",
            "âœ… ë§¤ì¹­ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: /content/drive/MyDrive/Robot_Analysis/simple_match_result.csv\n",
            "\n",
            "[ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°]\n",
            "   ì—í”¼ì†Œë“œ_ë²ˆí˜¸   AI_ì˜ˆì¸¡í™•ë¥   ì‹¤ì œ_ê²°ê³¼(1=ê³ ì¥)      íŒì •\n",
            "0        5  0.988862            1  O (ì •ë‹µ)\n",
            "1        6  0.989561            1  O (ì •ë‹µ)\n",
            "2        7  0.988293            1  O (ì •ë‹µ)\n",
            "3        8  0.912907            1  O (ì •ë‹µ)\n",
            "4       11  0.989370            1  O (ì •ë‹µ)\n",
            "5       12  0.808132            1  O (ì •ë‹µ)\n",
            "6       21  0.962442            1  O (ì •ë‹µ)\n",
            "7       24  0.989074            1  O (ì •ë‹µ)\n",
            "8       29  0.988117            1  O (ì •ë‹µ)\n",
            "9       30  0.964231            1  O (ì •ë‹µ)\n",
            "\n",
            "ğŸ“Š ìš”ì•½: AIê°€ ìœ„í—˜í•˜ë‹¤ê³  í•œ 657ê°œ ì¤‘ 575ê°œê°€ ì‹¤ì œë¡œ ê³ ì¥ë‚¨.\n",
            "   -> ì ì¤‘ë¥ : 87.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "el6h3E7XNF3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# ==========================================\n",
        "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "# ==========================================\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ“‚ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ì¤‘...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Robot_Analysis'\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "X_TEST_PATH = os.path.join(BASE_DIR, 'X_test.npy')\n",
        "MODEL_PATH = os.path.join(BASE_DIR, 'transformer_model.pt')\n",
        "OUTPUT_PATH = os.path.join(BASE_DIR, 'danger_max_duration_summary.csv') # ìµœì¢… ê²°ê³¼ íŒŒì¼ëª…\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„° (Min/Max ê¸°ì¤€ìš©)\n",
        "TRAIN_STEPS_PATH = os.path.join(BASE_DIR, 'train_steps.csv')\n",
        "TRAIN_SUMMARY_PATH = os.path.join(BASE_DIR, 'train_summary.csv')\n",
        "VAL_STEPS_PATH = os.path.join(BASE_DIR, 'val_steps.csv')\n",
        "VAL_SUMMARY_PATH = os.path.join(BASE_DIR, 'val_summary.csv')\n",
        "\n",
        "# ì„¤ì •\n",
        "INPUT_DIM = 9\n",
        "SEQ_LEN = 2400\n",
        "D_MODEL = 64\n",
        "NHEAD = 4\n",
        "NUM_LAYERS = 3\n",
        "DIM_FEEDFORWARD = 128\n",
        "DROPOUT = 0.1\n",
        "NUM_CLASSES = 2\n",
        "AI_THRESHOLD = 0.58\n",
        "\n",
        "# ì»¬ëŸ¼ ì´ë¦„ (CSV í—¤ë”ì™€ ì¼ì¹˜í•´ì•¼ í•¨)\n",
        "FEATURE_COLS = [\n",
        "    'speed', 'dist_to_wall', 'drag_force_N',\n",
        "    'mass_kg', 'friction_cond', 'air_density', 'trigger_dist_m', 'brake_torque', 'init_speed_cmd'\n",
        "]\n",
        "STATIC_COLS = [\n",
        "    'episode', 'mass_kg', 'friction_cond', 'air_density', 'brake_torque', 'init_speed_cmd'\n",
        "]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
        "# ==========================================\n",
        "class BasePredictionModel(nn.Module):\n",
        "    def forward(self, x): raise NotImplementedError\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "class TransformerModel(BasePredictionModel):\n",
        "    def __init__(self, input_dim, seq_len, num_classes=2, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1, use_patch=False, patch_len=10, patch_stride=5):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.use_patch = use_patch\n",
        "        self.patch_len = patch_len\n",
        "        self.patch_stride = patch_stride\n",
        "\n",
        "        if use_patch:\n",
        "            token_dim = patch_len * input_dim\n",
        "            if seq_len < patch_len:\n",
        "                num_tokens = 1\n",
        "            else:\n",
        "                num_tokens = 1 + (seq_len - patch_len) // patch_stride\n",
        "            self.num_tokens = num_tokens\n",
        "        else:\n",
        "            token_dim = input_dim\n",
        "            self.num_tokens = seq_len\n",
        "\n",
        "        self.input_proj = nn.Linear(token_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=self.num_tokens)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cls_head = nn.Linear(d_model, num_classes)\n",
        "        self.seq_head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x, return_sequence=True):\n",
        "        tokens = x\n",
        "        tokens = self.input_proj(tokens)\n",
        "        tokens = self.pos_encoder(tokens)\n",
        "        h = self.transformer_encoder(tokens)\n",
        "\n",
        "        if return_sequence:\n",
        "            logits_seq = self.seq_head(h)\n",
        "            probs_seq = torch.sigmoid(logits_seq)\n",
        "            return probs_seq\n",
        "        else:\n",
        "            h_mean = h.mean(dim=1)\n",
        "            h_mean = self.dropout(h_mean)\n",
        "            logits = self.cls_head(h_mean)\n",
        "            return logits\n",
        "\n",
        "# ==========================================\n",
        "# 3. ë°ì´í„° ë¡œë“œ ë° Min/Max ê³„ì‚°\n",
        "# ==========================================\n",
        "def load_and_merge(steps_path, summary_path):\n",
        "    if not os.path.exists(steps_path) or not os.path.exists(summary_path):\n",
        "        return None\n",
        "    df_steps = pd.read_csv(steps_path)\n",
        "    df_summary = pd.read_csv(summary_path)\n",
        "    df_merged = pd.merge(df_steps, df_summary[STATIC_COLS], on='episode', how='left')\n",
        "    return df_merged\n",
        "\n",
        "def calculate_global_minmax():\n",
        "    print(\"âš–ï¸ ì •ê·œí™” ê¸°ì¤€ê°’ ê³„ì‚° ì¤‘ (Train + Val ë³‘í•©)...\")\n",
        "\n",
        "    df_train = load_and_merge(TRAIN_STEPS_PATH, TRAIN_SUMMARY_PATH)\n",
        "    if df_train is None:\n",
        "        print(\"âŒ Train íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "        return None, None\n",
        "\n",
        "    df_val = load_and_merge(VAL_STEPS_PATH, VAL_SUMMARY_PATH)\n",
        "\n",
        "    if df_val is not None:\n",
        "        df_all = pd.concat([df_train, df_val], axis=0, ignore_index=True)\n",
        "    else:\n",
        "        df_all = df_train\n",
        "\n",
        "    try:\n",
        "        target_data = df_all[FEATURE_COLS]\n",
        "        min_vals = target_data.min().values\n",
        "        max_vals = target_data.max().values\n",
        "    except KeyError as e:\n",
        "        print(f\"âŒ ì»¬ëŸ¼ ì´ë¦„ ì˜¤ë¥˜: {e}\")\n",
        "        print(f\"   CSV ì»¬ëŸ¼: {list(df_all.columns)}\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"\\nğŸ“Š [MinMax ê¸°ì¤€ê°’]\")\n",
        "    for i, col in enumerate(FEATURE_COLS):\n",
        "        print(f\"   - {col}: Min={min_vals[i]:.4f}, Max={max_vals[i]:.4f}\")\n",
        "\n",
        "    return min_vals, max_vals\n",
        "\n",
        "# ==========================================\n",
        "# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def main():\n",
        "    # (1) ê¸°ì¤€ê°’ ê³„ì‚°\n",
        "    data_min, data_max = calculate_global_minmax()\n",
        "    if data_min is None: return\n",
        "\n",
        "    data_range = data_max - data_min\n",
        "    data_range[data_range == 0] = 1.0\n",
        "\n",
        "    # (2) ì…ë ¥ ë°ì´í„° ë¡œë“œ\n",
        "    if not os.path.exists(X_TEST_PATH):\n",
        "        print(f\"âŒ npy íŒŒì¼ ì—†ìŒ: {X_TEST_PATH}\")\n",
        "        return\n",
        "\n",
        "    X_test_np = np.load(X_TEST_PATH)\n",
        "    print(f\"\\nğŸ“‚ ì…ë ¥ ë°ì´í„°(npy) ë¡œë“œ ì™„ë£Œ: {X_test_np.shape}\")\n",
        "    X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # (3) ëª¨ë¸ ì˜ˆì¸¡\n",
        "    print(\"\\nğŸ¤– Transformer ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡...\")\n",
        "    model = TransformerModel(input_dim=INPUT_DIM, seq_len=SEQ_LEN, num_classes=NUM_CLASSES, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, use_patch=False).to(device)\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "        model.eval()\n",
        "    else:\n",
        "        print(\"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ\")\n",
        "        return\n",
        "\n",
        "    print(\"ğŸ”® ìœ„í—˜ë„ ì˜ˆì¸¡ ì¤‘...\")\n",
        "    BATCH_SIZE = 32\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_samples = X_test_tensor.shape[0]\n",
        "        for i in range(0, num_samples, BATCH_SIZE):\n",
        "            batch_x = X_test_tensor[i : i+BATCH_SIZE]\n",
        "            batch_pred = model(batch_x, return_sequence=True)\n",
        "            all_preds.append(batch_pred.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(all_preds, axis=0).squeeze(-1)\n",
        "\n",
        "    # (4) â˜… í•µì‹¬: ìµœì¥ ìœ„í—˜ êµ¬ê°„ì˜ ì‹œì‘ì  ì°¾ê¸° â˜…\n",
        "    print(f\"\\nğŸ” ì—í”¼ì†Œë“œë³„ 'ê°€ì¥ ê¸¸ê²Œ ì§€ì†ëœ' ìœ„í—˜ êµ¬ê°„ ì¶”ì¶œ ì¤‘...\")\n",
        "\n",
        "    extracted_rows = []\n",
        "\n",
        "    for ep_idx, episode_probs in enumerate(predictions):\n",
        "        # ìœ„í—˜ ë§ˆìŠ¤í¬ (True/False)\n",
        "        is_danger = episode_probs >= AI_THRESHOLD\n",
        "\n",
        "        # ìƒíƒœ ë³€í™” ì§€ì  ì°¾ê¸° (Start/End)\n",
        "        padded = np.concatenate(([False], is_danger, [False]))\n",
        "        diff = np.diff(padded.astype(int))\n",
        "\n",
        "        starts = np.where(diff == 1)[0]\n",
        "        ends = np.where(diff == -1)[0]\n",
        "\n",
        "        # ìœ„í—˜ êµ¬ê°„ì´ í•˜ë‚˜ë¼ë„ ìˆë‹¤ë©´\n",
        "        if len(starts) > 0:\n",
        "            # ê° êµ¬ê°„ì˜ ê¸¸ì´(Duration) ê³„ì‚°\n",
        "            durations = ends - starts\n",
        "\n",
        "            # ê°€ì¥ ê¸´ êµ¬ê°„ì˜ ì¸ë±ìŠ¤ ì°¾ê¸° (Argmax)\n",
        "            longest_idx = np.argmax(durations)\n",
        "\n",
        "            # ê·¸ êµ¬ê°„ì˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "            best_start = starts[longest_idx]\n",
        "            best_duration = durations[longest_idx]\n",
        "            best_prob = np.mean(episode_probs[best_start : ends[longest_idx]]) # êµ¬ê°„ í‰ê·  í™•ë¥ \n",
        "\n",
        "            # ì‹œì‘ ì‹œì ì˜ ë¬¼ë¦¬ê°’ ë³µì›\n",
        "            norm_vals = X_test_np[ep_idx, best_start, :]\n",
        "            real_vals = norm_vals * data_range + data_min\n",
        "\n",
        "            # [ì—í”¼ì†Œë“œ, ì‹œì‘ì‹œê°„, ì§€ì†ì‹œê°„, í‰ê· í™•ë¥ , ...ë¬¼ë¦¬ë³€ìˆ˜ë“¤...]\n",
        "            row = [ep_idx, best_start, best_duration, best_prob] + real_vals.tolist()\n",
        "            extracted_rows.append(row)\n",
        "\n",
        "    # (5) ì €ì¥\n",
        "    if len(extracted_rows) > 0:\n",
        "        result_df = pd.DataFrame(extracted_rows, columns=[\"Episode_ID\", \"Start_Time\", \"Max_Duration\", \"Avg_Prob\"] + FEATURE_COLS)\n",
        "\n",
        "        result_df.to_csv(OUTPUT_PATH, index=False)\n",
        "        print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ! ì´ {len(result_df)}ê±´ (ì—í”¼ì†Œë“œë‹¹ 1ê°œ) ì €ì¥ë¨.\")\n",
        "        print(f\"ğŸ’¾ íŒŒì¼ ê²½ë¡œ: {OUTPUT_PATH}\")\n",
        "        print(result_df.head())\n",
        "\n",
        "        # (6) ìë™ ë¶„ì„ (Risk Score)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ“Š [ìë™ ë¶„ì„] ìµœì¥ ìœ„í—˜ êµ¬ê°„ ì‹œì‘ì ì˜ í†µê³„\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        dists = result_df['dist_to_wall'].replace(0, 0.01)\n",
        "        frictions = result_df['friction_cond'].replace(0, 0.01)\n",
        "\n",
        "        # Risk = Speed^2 / (Friction * Dist)\n",
        "        risk_scores = (result_df['speed'] ** 2) / (frictions * dists)\n",
        "\n",
        "        th_10 = np.percentile(risk_scores, 10)\n",
        "\n",
        "        print(f\"í‰ê·  Risk Score: {risk_scores.mean():.2f}\")\n",
        "        print(f\"ğŸš¨ ì¶”ì²œ ê²½ë³´ ì„ê³„ê°’ (í•˜ìœ„ 10%): {th_10:.2f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ 0.58ì„ ë„˜ëŠ” ìœ„í—˜ êµ¬ê°„ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Bv18LTLSCl",
        "outputId": "d1ab078e-cbee-48dd-85d7-7319a2402027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš–ï¸ ì •ê·œí™” ê¸°ì¤€ê°’ ê³„ì‚° ì¤‘ (Train + Val ë³‘í•©)...\n",
            "\n",
            "ğŸ“Š [MinMax ê¸°ì¤€ê°’]\n",
            "   - speed: Min=0.0053, Max=2.5404\n",
            "   - dist_to_wall: Min=0.0000, Max=6.3000\n",
            "   - drag_force_N: Min=0.0000, Max=0.1724\n",
            "   - mass_kg: Min=4.0002, Max=7.9996\n",
            "   - friction_cond: Min=0.1000, Max=1.0000\n",
            "   - air_density: Min=1.1500, Max=1.3500\n",
            "   - trigger_dist_m: Min=0.0500, Max=0.3000\n",
            "   - brake_torque: Min=2.0000, Max=2.0000\n",
            "   - init_speed_cmd: Min=60.0017, Max=79.9995\n",
            "\n",
            "ğŸ“‚ ì…ë ¥ ë°ì´í„°(npy) ë¡œë“œ ì™„ë£Œ: (2033, 2400, 9)\n",
            "\n",
            "ğŸ¤– Transformer ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡...\n",
            "ğŸ”® ìœ„í—˜ë„ ì˜ˆì¸¡ ì¤‘...\n",
            "\n",
            "ğŸ” ì—í”¼ì†Œë“œë³„ 'ê°€ì¥ ê¸¸ê²Œ ì§€ì†ëœ' ìœ„í—˜ êµ¬ê°„ ì¶”ì¶œ ì¤‘...\n",
            "\n",
            "âœ… ë¶„ì„ ì™„ë£Œ! ì´ 657ê±´ (ì—í”¼ì†Œë“œë‹¹ 1ê°œ) ì €ì¥ë¨.\n",
            "ğŸ’¾ íŒŒì¼ ê²½ë¡œ: /content/drive/MyDrive/Robot_Analysis/danger_max_duration_summary.csv\n",
            "   Episode_ID  Start_Time  Max_Duration  Avg_Prob     speed  dist_to_wall  \\\n",
            "0           5           0          1726  0.988862  0.416270      6.300002   \n",
            "1           6           0          2152  0.989561  0.416270      6.300001   \n",
            "2           7           0          1664  0.988293  0.416270      6.300000   \n",
            "3           8           0          2400  0.912907  0.416270      6.300002   \n",
            "4          11           0          1568  0.989370  0.416271      6.300001   \n",
            "\n",
            "   drag_force_N   mass_kg  friction_cond  air_density  trigger_dist_m  \\\n",
            "0      0.004347  4.529247            0.5        1.150        0.103558   \n",
            "1      0.004347  5.850512            0.5        1.150        0.095871   \n",
            "2      0.005102  7.705468            0.1        1.350        0.292249   \n",
            "3      0.004630  7.458602            1.0        1.225        0.170339   \n",
            "4      0.004630  4.271843            1.0        1.225        0.067129   \n",
            "\n",
            "   brake_torque  init_speed_cmd  \n",
            "0           2.0       73.379556  \n",
            "1           2.0       62.803646  \n",
            "2           2.0       76.720552  \n",
            "3           2.0       68.128281  \n",
            "4           2.0       77.026851  \n",
            "\n",
            "============================================================\n",
            "ğŸ“Š [ìë™ ë¶„ì„] ìµœì¥ ìœ„í—˜ êµ¬ê°„ ì‹œì‘ì ì˜ í†µê³„\n",
            "============================================================\n",
            "í‰ê·  Risk Score: 27.72\n",
            "ğŸš¨ ì¶”ì²œ ê²½ë³´ ì„ê³„ê°’ (í•˜ìœ„ 10%): 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrxwPRI-fE2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}