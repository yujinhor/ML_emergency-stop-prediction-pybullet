import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from tqdm import tqdm
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler  # í‘œì¤€í™” ë„êµ¬ ì¶”ê°€
import sys
import os

# ----------------------------------------------------
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •
# ----------------------------------------------------
INPUT_SIZE = 8        # Brake_Torque ì œê±°ë¡œ 8ê°œ
HIDDEN_SIZE = 64      
NUM_LAYERS = 2        
OUTPUT_SIZE = 1       

SEQ_LEN = 2400        
BATCH_SIZE = 32       
LEARNING_RATE = 0.001
EPOCHS = 20
TIME_PENALTY = 1.0 

POS_WEIGHT_VAL = 2.0  # í™©ê¸ˆ ë¹„ìœ¨ ìœ ì§€

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.backends.mps.is_available(): 
    device = torch.device("mps")
    print("ğŸš€ Mac MPS ê°€ì† í™œì„±í™”ë¨")
else:
    print(f"Using device: {device}")


# ----------------------------------------------------
# 2. ì†ì‹¤í•¨ìˆ˜
# ----------------------------------------------------
class TimeAwareBCELoss(nn.Module):
    def __init__(self, time_penalty_weight=1.0, pos_weight=1.0):
        super(TimeAwareBCELoss, self).__init__()
        self.time_penalty_weight = time_penalty_weight
        self.pos_weight = pos_weight

    def forward(self, predictions, targets, inputs):
        if targets.dim() == 1: targets = targets.view(-1, 1)
        targets_expanded = targets.unsqueeze(1).expand_as(predictions)
        
        weights = torch.ones_like(targets_expanded)
        weights[targets_expanded == 1] = self.pos_weight
        
        bce_loss = F.binary_cross_entropy(predictions, targets_expanded, weight=weights, reduction='none')
        
        batch_size, seq_len, _ = predictions.shape
        time_steps = torch.linspace(0, 1, seq_len, device=predictions.device).view(1, -1, 1)
        time_weights = 1.0 + (self.time_penalty_weight * time_steps)
        weighted_loss = bce_loss * time_weights
        
        mask = (torch.abs(inputs).sum(dim=-1, keepdim=True) > 0).float()
        masked_loss = weighted_loss * mask
        
        loss = masked_loss.sum() / (mask.sum() + 1e-8)
        return loss


# ----------------------------------------------------
# 3. ë°ì´í„° ë¡œë”© ë° í‘œì¤€í™” (í•µì‹¬ ë³€ê²½!)
# ----------------------------------------------------
def load_and_scale_data():
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    X_train_np = np.load('X_train.npy')
    X_test_np = np.load('X_test.npy')
    
    # 1. ì£½ì€ ë³€ìˆ˜(Brake_Torque, 7ë²ˆ) ì œê±°
    print("âœ‚ï¸ 'Brake_Torque' ë³€ìˆ˜ ì œê±° ì¤‘...")
    X_train_np = np.delete(X_train_np, 7, axis=2)
    X_test_np = np.delete(X_test_np, 7, axis=2)
    
    # 2. í‘œì¤€í™” (StandardScaler) ì ìš©
    # 3ì°¨ì›(Sample, Time, Feature) -> 2ì°¨ì›(Sample*Time, Feature)ë¡œ í´ì„œ ìŠ¤ì¼€ì¼ë§ í›„ ë‹¤ì‹œ ë³µêµ¬
    print("âš–ï¸ ë°ì´í„° í‘œì¤€í™”(StandardScaler) ì ìš© ì¤‘...")
    
    scaler = StandardScaler()
    
    N, T, F = X_train_np.shape
    # Train ë°ì´í„°ë¡œ í”¼íŒ… (í‰ê· , ë¶„ì‚° ê³„ì‚°)
    X_train_2d = X_train_np.reshape(N * T, F)
    X_train_scaled = scaler.fit_transform(X_train_2d)
    X_train_np = X_train_scaled.reshape(N, T, F) # ë‹¤ì‹œ 3ì°¨ì›ìœ¼ë¡œ ë³µêµ¬
    
    # Test ë°ì´í„°ëŠ” Trainì˜ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜ë§Œ (Transform)
    N_test, T_test, _ = X_test_np.shape
    X_test_2d = X_test_np.reshape(N_test * T_test, F)
    X_test_scaled = scaler.transform(X_test_2d)
    X_test_np = X_test_scaled.reshape(N_test, T_test, F)
    
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    
    # Tensor ë³€í™˜
    X_train = torch.tensor(X_train_np, dtype=torch.float32)
    X_test = torch.tensor(X_test_np, dtype=torch.float32)
    y_train = torch.tensor(np.load('y_train.npy'), dtype=torch.float32).unsqueeze(1)
    y_test = torch.tensor(np.load('y_test.npy'), dtype=torch.float32).unsqueeze(1)
    
    return X_train, y_train, X_test, y_test

# ì‹¤í–‰
X_train, y_train, X_test, y_test = load_and_scale_data()

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)


# ----------------------------------------------------
# 4. ëª¨ë¸ ì •ì˜
# ----------------------------------------------------
class RobotGRU(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RobotGRU, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.gru(x, h0)
        logits = self.fc(out)
        probs = torch.sigmoid(logits) 
        return probs

model = RobotGRU(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE).to(device)


# ----------------------------------------------------
# 5. í•™ìŠµ ë£¨í”„
# ----------------------------------------------------
criterion = TimeAwareBCELoss(time_penalty_weight=TIME_PENALTY, pos_weight=POS_WEIGHT_VAL)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

best_f1 = 0.0
best_model_path = 'gru_model_best.pth'
final_model_path = 'gru_model_last.pth'

def train_epoch(model, loader, epoch_idx):
    model.train()
    total_loss = 0
    loop = tqdm(loader, desc=f"Epoch {epoch_idx+1} Train", leave=False)
    
    for inputs, targets in loop:
        inputs_dev, targets_dev = inputs.to(device), targets.to(device)
        outputs = model(inputs_dev)
        loss = criterion(outputs.cpu(), targets.cpu(), inputs.cpu())
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())
    
    return total_loss / len(loader)

def evaluate(model, loader):
    model.eval()
    total_loss = 0
    all_targets = []
    all_preds = []
    
    with torch.no_grad():
        for inputs, targets in loader:
            inputs_dev = inputs.to(device)
            outputs = model(inputs_dev)
            
            loss = criterion(outputs.cpu(), targets.cpu(), inputs.cpu())
            total_loss += loss.item()
            
            final_preds = (outputs[:, -1, :] > 0.5).float().cpu()
            all_preds.extend(final_preds.numpy())
            all_targets.extend(targets.numpy())
            
    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)
    
    acc = 100 * np.mean(all_preds == all_targets)
    f1 = f1_score(all_targets, all_preds, zero_division=0)
    
    return total_loss / len(loader), acc, f1

print("\n[Training Start] í•™ìŠµ ì‹œì‘ (Brake ì œê±° + í‘œì¤€í™” + ê°€ì¤‘ì¹˜)...")

for epoch in range(EPOCHS):
    train_loss = train_epoch(model, train_loader, epoch)
    test_loss, test_acc, test_f1 = evaluate(model, test_loader)
    
    print(f"Epoch [{epoch+1}/{EPOCHS}] | "
          f"Loss: {train_loss:.4f} | "
          f"Acc: {test_acc:.2f}% | "
          f"F1 Score: {test_f1:.4f}")
    
    if test_f1 > best_f1:
        best_f1 = test_f1
        torch.save(model.state_dict(), best_model_path)
        print(f"  --> ğŸ‰ F1 ìµœê³ ì  ê°±ì‹ ! ëª¨ë¸ ì €ì¥ë¨ (F1: {best_f1:.4f})")

print("\nâœ… í•™ìŠµ ì¢…ë£Œ.")
torch.save(model.state_dict(), final_model_path)
print(f"ğŸ’¾ ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥ë¨: {final_model_path}")