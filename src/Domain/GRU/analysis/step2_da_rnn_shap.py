import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import sys
import os

# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ----------------------------------------------------
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
device = torch.device("cpu") # SHAP ê³„ì‚° ì‹œ CPU ê¶Œì¥

INPUT_SIZE = 8
TIME_STEPS = 2400 
FEATURE_COUNT = 8 
HIDDEN_SIZE = 64 # DA-RNN Hidden Size

feature_names = [
    "Speed", "Dist_to_Wall", "Drag_Force", "Is_Braking", 
    "Mass_kg", "Friction_Cond", "Air_Density", "Init_Speed"
]

def load_data():
    print("ğŸ“‚ [DA-RNN] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì¤‘...")
    try:
        X_train_np = np.load('X_train.npy')
        X_test_np = np.load('X_test.npy')
    except FileNotFoundError:
        print("âŒ ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit()

    if X_test_np.shape[2] > 8:
        X_train_np = np.delete(X_train_np, 7, axis=2)
        X_test_np = np.delete(X_test_np, 7, axis=2)

    scaler = StandardScaler()
    
    # [ìˆ˜ì •ëœ ë¶€ë¶„] Fë¼ëŠ” ë³€ìˆ˜ëª… ëŒ€ì‹  n_featsë¥¼ ì‚¬ìš©í•´ ì¶©ëŒ ë°©ì§€
    N, T, n_feats = X_train_np.shape 
    X_train_2d = X_train_np.reshape(N * T, n_feats)
    scaler.fit(X_train_2d)

    N_test, T_test, _ = X_test_np.shape
    X_test_2d = X_test_np.reshape(N_test * T_test, n_feats)
    X_test_scaled = scaler.transform(X_test_2d)
    
    X_test_final = X_test_scaled.reshape(N_test, T_test, n_feats)
    return torch.tensor(X_test_final, dtype=torch.float32).to(device)

X_test_tensor = load_data()
total_samples = X_test_tensor.shape[0]
print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! (ì´ {total_samples}ê°œ)")


# ----------------------------------------------------
# 2. DA-RNN ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ----------------------------------------------------
class InputAttention(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(InputAttention, self).__init__()
        self.linear = nn.Linear(hidden_size + input_size, input_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x_t, h_prev):
        combined = torch.cat((x_t, h_prev), dim=1)
        scores = torch.tanh(self.linear(combined))
        alpha = self.softmax(scores) 
        return alpha * x_t, alpha

class TemporalAttention(nn.Module):
    def __init__(self, hidden_size):
        super(TemporalAttention, self).__init__()
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, encoder_outputs):
        scores = torch.tanh(self.linear(encoder_outputs))
        # ì—¬ê¸°ì„œ FëŠ” ì´ì œ ì •ìƒì ìœ¼ë¡œ torch.nn.functionalì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
        beta = F.softmax(scores, dim=1) 
        context = torch.sum(beta * encoder_outputs, dim=1)
        return context, beta

class DA_RNN_Explainable(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DA_RNN_Explainable, self).__init__()
        self.hidden_size = hidden_size
        self.input_attention = InputAttention(input_size, hidden_size)
        self.gru_cell = nn.GRUCell(input_size, hidden_size)
        self.temporal_attention = TemporalAttention(hidden_size)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
        encoder_outputs = []
        alpha_list = []
        
        for t in range(seq_len):
            x_t = x[:, t, :]
            weighted_x_t, alpha = self.input_attention(x_t, h_t)
            h_t = self.gru_cell(weighted_x_t, h_t)
            encoder_outputs.append(h_t.unsqueeze(1))
            alpha_list.append(alpha.unsqueeze(1))
            
        encoder_outputs = torch.cat(encoder_outputs, dim=1)
        input_attention_weights = torch.cat(alpha_list, dim=1)
        context_vector, beta = self.temporal_attention(encoder_outputs)
        logits = self.fc(context_vector)
        
        return torch.sigmoid(logits), input_attention_weights, beta

print("ğŸ“‚ DA-RNN ëª¨ë¸ ë¡œë”© ì¤‘...")
model = DA_RNN_Explainable(INPUT_SIZE, HIDDEN_SIZE, 1).to(device)

# ëª¨ë¸ íŒŒì¼ëª… í™•ì¸ (ì˜ˆ: da_rnn_best_maxpool.pth)
try:
    model.load_state_dict(torch.load('da_rnn_best_maxpool.pth', map_location=device))
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
except:
    print("âš ï¸ íŒŒì¼ëª… í™•ì¸ í•„ìš”: 'da_rnn_best_maxpool.pth'ê°€ ì—†ì–´ì„œ 'da_rnn_best.pth' ì‹œë„")
    try:
        model.load_state_dict(torch.load('da_rnn_best.pth', map_location=device))
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except:
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit()


# ----------------------------------------------------
# 3. [ì¤‘ìš”] DA-RNN ì „ìš© Wrapper í•¨ìˆ˜
# ----------------------------------------------------
def predict_func_darnn(X_np):
    model.eval()
    N_samples = X_np.shape[0]
    X_reshaped = X_np.reshape(N_samples, TIME_STEPS, FEATURE_COUNT) 
    X_tensor = torch.tensor(X_reshaped, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        outputs = model(X_tensor)
        logits = outputs[0]  # [0]ë²ˆ ì¸ë±ìŠ¤ê°€ ì˜ˆì¸¡ê°’
        
        if logits.dim() == 3: output = logits[:, -1, :]
        elif logits.dim() == 2: output = logits
        else: output = logits
        
        return output.cpu().numpy().flatten()


# ----------------------------------------------------
# 4. Global SHAP ë¶„ì„ (N=100, ë°°ì¹˜ ì‹¤í–‰)
# ----------------------------------------------------
print("\nğŸ“Š DA-RNN Global SHAP ë¶„ì„ ì‹œì‘ (ëª©í‘œ: 100ê°œ)")
import warnings
warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ë„ê¸°

def flatten_data(tensor):
    return tensor.cpu().numpy().reshape(tensor.shape[0], -1)

# ë°°ê²½ ë°ì´í„°
idx_bg = np.random.choice(total_samples, 20, replace=False)
background_np_flat = flatten_data(X_test_tensor[idx_bg])

# Explainer ìƒì„±
explainer = shap.KernelExplainer(predict_func_darnn, background_np_flat)

# íƒ€ê²Ÿ ìƒ˜í”Œ ì„ ì •
TARGET_N = 100
BATCH_SIZE = 10
if total_samples < TARGET_N: TARGET_N = total_samples
all_indices = np.random.choice(total_samples, TARGET_N, replace=False)

all_shap_values = []
all_test_data = []

for i in range(0, TARGET_N, BATCH_SIZE):
    current_indices = all_indices[i : i + BATCH_SIZE]
    batch_tensor = X_test_tensor[current_indices]
    batch_flat = flatten_data(batch_tensor)
    
    # SHAP ê³„ì‚°
    shap_vals_batch = explainer.shap_values(batch_flat, nsamples=100, silent=True)
    if isinstance(shap_vals_batch, list): shap_vals_batch = shap_vals_batch[0]
        
    all_shap_values.append(shap_vals_batch)
    all_test_data.append(batch_flat)
    print(f"   Running... [{i + len(current_indices)} / {TARGET_N} ì™„ë£Œ]")

# ê²°ê³¼ í†µí•©
shap_vals_total = np.concatenate(all_shap_values, axis=0)
test_data_total = np.concatenate(all_test_data, axis=0)

# ----------------------------------------------------
# 5. ì‹œê°í™” ë° ì €ì¥
# ----------------------------------------------------
print("\nğŸ“ˆ DA-RNN SHAP ê²°ê³¼ ì €ì¥ ì¤‘...")

# 3D ë³€í™˜ ë° í‰ê· 
shap_vals_3d = shap_vals_total.reshape(-1, TIME_STEPS, FEATURE_COUNT)
shap_vals_mean = shap_vals_3d.mean(axis=1)
test_data_3d = test_data_total.reshape(-1, TIME_STEPS, FEATURE_COUNT)
test_data_mean = test_data_3d.mean(axis=1)

# Bar Plot
plt.figure()
shap.summary_plot(shap_vals_mean, test_data_mean, feature_names=feature_names, plot_type="bar", show=False)
plt.title("DA-RNN ëª¨ë¸ ì „ì²´ ë³€ìˆ˜ ì¤‘ìš”ë„ (SHAP)")
plt.tight_layout()
plt.savefig('darnn_global_shap_bar.png', dpi=300)
print("  --> ì €ì¥ ì™„ë£Œ: darnn_global_shap_bar.png")

# Dot Plot
plt.figure()
shap.summary_plot(shap_vals_mean, test_data_mean, feature_names=feature_names, show=False)
plt.title("DA-RNN ëª¨ë¸ ë³€ìˆ˜ ì˜í–¥ë ¥ ë¶„í¬ (SHAP)")
plt.tight_layout()
plt.savefig('darnn_global_shap_dot.png', dpi=300)
print("  --> ì €ì¥ ì™„ë£Œ: darnn_global_shap_dot.png")

print("\nğŸ‰ DA-RNN ë¶„ì„ ì™„ë£Œ!")