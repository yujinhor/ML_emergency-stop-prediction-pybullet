import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import sys
import os

# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ----------------------------------------------------
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
device = torch.device("cpu") # Attention ì¶”ì¶œì€ CPUë¡œë„ ìˆœì‹ê°„ì— ëë‚©ë‹ˆë‹¤.

INPUT_SIZE = 8
HIDDEN_SIZE = 64 

feature_names = [
    "Speed", "Dist_to_Wall", "Drag_Force", "Is_Braking", 
    "Mass_kg", "Friction_Cond", "Air_Density", "Init_Speed"
]

# ----------------------------------------------------
# 2. ë°ì´í„° ì¤€ë¹„
# ----------------------------------------------------
print("ğŸ“‚ [Attention] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì¤‘...")
try:
    X_train_np = np.load('X_train.npy')
    X_test_np = np.load('X_test.npy')
except FileNotFoundError:
    print("âŒ ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()

if X_train_np.shape[2] > 8:
    X_train_np = np.delete(X_train_np, 7, axis=2)
    X_test_np = np.delete(X_test_np, 7, axis=2)

scaler = StandardScaler()

# [ìˆ˜ì •ë¨] F ëŒ€ì‹  n_featsë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶©ëŒ ë°©ì§€
N, T, n_feats = X_train_np.shape
X_train_2d = X_train_np.reshape(N * T, n_feats)
scaler.fit(X_train_2d)

N_test, T_test, _ = X_test_np.shape
X_test_2d = X_test_np.reshape(N_test * T_test, n_feats)
X_test_scaled = scaler.transform(X_test_2d)

X_test_final = X_test_scaled.reshape(N_test, T_test, n_feats)
X_test_tensor = torch.tensor(X_test_final, dtype=torch.float32).to(device)
print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (ì´ {N_test}ê°œ ìƒ˜í”Œ)")

# ----------------------------------------------------
# 3. ëª¨ë¸ ì •ì˜ (DA-RNN)
# ----------------------------------------------------
class InputAttention(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(InputAttention, self).__init__()
        self.linear = nn.Linear(hidden_size + input_size, input_size)
        self.softmax = nn.Softmax(dim=1)
    def forward(self, x_t, h_prev):
        combined = torch.cat((x_t, h_prev), dim=1)
        scores = torch.tanh(self.linear(combined))
        alpha = self.softmax(scores) 
        return alpha * x_t, alpha

class TemporalAttention(nn.Module):
    def __init__(self, hidden_size):
        super(TemporalAttention, self).__init__()
        self.linear = nn.Linear(hidden_size, 1)
    def forward(self, encoder_outputs):
        scores = torch.tanh(self.linear(encoder_outputs))
        # ì—¬ê¸°ì„œ FëŠ” ì´ì œ ì•ˆì „í•˜ê²Œ torch.nn.functionalì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
        beta = F.softmax(scores, dim=1) 
        context = torch.sum(beta * encoder_outputs, dim=1)
        return context, beta

class DA_RNN_Explainable(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DA_RNN_Explainable, self).__init__()
        self.hidden_size = hidden_size
        self.input_attention = InputAttention(input_size, hidden_size)
        self.gru_cell = nn.GRUCell(input_size, hidden_size)
        self.temporal_attention = TemporalAttention(hidden_size)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)
        encoder_outputs = []
        alpha_list = []
        for t in range(seq_len):
            x_t = x[:, t, :]
            weighted_x_t, alpha = self.input_attention(x_t, h_t)
            h_t = self.gru_cell(weighted_x_t, h_t)
            encoder_outputs.append(h_t.unsqueeze(1))
            alpha_list.append(alpha.unsqueeze(1))
        encoder_outputs = torch.cat(encoder_outputs, dim=1)
        input_attention_weights = torch.cat(alpha_list, dim=1)
        context_vector, beta = self.temporal_attention(encoder_outputs)
        logits = self.fc(context_vector)
        return torch.sigmoid(logits), input_attention_weights, beta

print("ğŸ“‚ DA-RNN ëª¨ë¸ ë¡œë”© ì¤‘...")
model = DA_RNN_Explainable(INPUT_SIZE, HIDDEN_SIZE, 1).to(device)

# ëª¨ë¸ íŒŒì¼ ë¡œë“œ (Step 2ì™€ ë™ì¼í•œ ë¡œì§)
try:
    model.load_state_dict(torch.load('da_rnn_best_maxpool.pth', map_location=device))
    model.eval()
    print("âœ… DA-RNN ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
except:
    print("âš ï¸ 'da_rnn_best_maxpool.pth' ì‹¤íŒ¨ -> 'da_rnn_best.pth' ì‹œë„")
    try:
        model.load_state_dict(torch.load('da_rnn_best.pth', map_location=device))
        print("âœ… DA-RNN ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except:
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit()

# ----------------------------------------------------
# 4. Global Attention ê³„ì‚°
# ----------------------------------------------------
print("\nğŸ§  Global Attention ê³„ì‚° ì‹œì‘ (ëª©í‘œ: 100ê°œ ìƒ˜í”Œ í‰ê· )...")

TARGET_N = 100
if N_test < TARGET_N: TARGET_N = N_test

# ëœë¤í•˜ê²Œ 100ê°œ ì¶”ì¶œ (ë˜ëŠ” ì „ì²´)
indices = np.random.choice(N_test, TARGET_N, replace=False)
selected_data = X_test_tensor[indices]

with torch.no_grad():
    # 2ë²ˆì§¸ ë¦¬í„´ê°’ì´ Input Attention Weightsì…ë‹ˆë‹¤.
    # (Batch, Time, Features)
    _, input_att_weights, _ = model(selected_data)

# ì „ì²´ í‰ê·  (Global Importance)
# 1. Time ì¶• í‰ê· : ì‹œê³„ì—´ ì „ì²´ì—ì„œ ë³€ìˆ˜ì˜ ê¸°ì—¬ë„
# 2. Batch ì¶• í‰ê· : ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ í‰ê· 
global_att_importance = input_att_weights.mean(dim=1).mean(dim=0).cpu().numpy()

print("âœ… ê³„ì‚° ì™„ë£Œ!")

# ----------------------------------------------------
# 5. ì‹œê°í™” ë° ì €ì¥
# ----------------------------------------------------
plt.figure(figsize=(10, 6))

# DA-RNN Attentionì€ ë³´ë¼ìƒ‰ ê³„ì—´ë¡œ í‘œì‹œ (SHAPê³¼ êµ¬ë¶„)
bars = plt.bar(feature_names, global_att_importance, color='rebeccapurple', alpha=0.8)

plt.title(f"[DA-RNN] Global Input Attention Importance (N={TARGET_N})")
plt.ylabel("Mean Attention Weight")
plt.xlabel("Features")
plt.grid(axis='y', alpha=0.3)

# ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.4f}', 
             va='bottom', ha='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('darnn_global_attention.png', dpi=300)
print("ğŸ‰ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: darnn_global_attention.png")
plt.show()